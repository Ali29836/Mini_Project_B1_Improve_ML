{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62gkk4_lKztu"
   },
   "source": [
    "# EEC 193A Pre Project B1: Improve your ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc654drGKztv"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NYkq2beKztw"
   },
   "source": [
    "This asignment follows Lab2 and uses the same dataset as Lab2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBtNuNPcKztx"
   },
   "source": [
    "### Reading Data into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "WKZVXLZfKztx"
   },
   "outputs": [],
   "source": [
    "# make sure we can plot in future if we want\n",
    "%matplotlib notebook\n",
    "# make sure to ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "# Import statement for pandas\n",
    "import pandas as pd\n",
    "# This is just a small configuration change for purposes of the class\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "# Get our train X and y datasets for the problem\n",
    "train_x = pd.read_csv('ece193a_pva_train_x.csv')\n",
    "train_y = pd.read_csv('ece193a_pva_train_y.csv')\n",
    "\n",
    "# Get our validation X and y datasets for the problem. \n",
    "test_x = pd.read_csv('ece193a_pva_validation_x.csv')\n",
    "test_y = pd.read_csv('ece193a_pva_validation_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "mRttEP9VKzt0",
    "outputId": "e05f6f7b-5145-4aee-a987-5bc1ac281680"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breath_id</th>\n",
       "      <th>i_time</th>\n",
       "      <th>tve</th>\n",
       "      <th>max_flow</th>\n",
       "      <th>min_flow</th>\n",
       "      <th>max_pressure</th>\n",
       "      <th>peep</th>\n",
       "      <th>ip_auc</th>\n",
       "      <th>ep_auc</th>\n",
       "      <th>patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>545.032222</td>\n",
       "      <td>51.06</td>\n",
       "      <td>-41.03</td>\n",
       "      <td>17.37</td>\n",
       "      <td>7.600</td>\n",
       "      <td>11.122367</td>\n",
       "      <td>16.057733</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>531.880278</td>\n",
       "      <td>53.13</td>\n",
       "      <td>-39.97</td>\n",
       "      <td>17.13</td>\n",
       "      <td>7.508</td>\n",
       "      <td>11.077750</td>\n",
       "      <td>17.310533</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>523.876667</td>\n",
       "      <td>52.86</td>\n",
       "      <td>-38.24</td>\n",
       "      <td>17.11</td>\n",
       "      <td>7.658</td>\n",
       "      <td>12.066000</td>\n",
       "      <td>16.697800</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.80</td>\n",
       "      <td>507.636111</td>\n",
       "      <td>51.04</td>\n",
       "      <td>-39.37</td>\n",
       "      <td>17.14</td>\n",
       "      <td>7.572</td>\n",
       "      <td>11.097800</td>\n",
       "      <td>15.774250</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>518.618889</td>\n",
       "      <td>47.88</td>\n",
       "      <td>-38.51</td>\n",
       "      <td>16.92</td>\n",
       "      <td>7.598</td>\n",
       "      <td>11.065400</td>\n",
       "      <td>18.483333</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>296</td>\n",
       "      <td>0.90</td>\n",
       "      <td>355.365278</td>\n",
       "      <td>42.26</td>\n",
       "      <td>-51.51</td>\n",
       "      <td>23.53</td>\n",
       "      <td>13.194</td>\n",
       "      <td>19.216400</td>\n",
       "      <td>21.816367</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>297</td>\n",
       "      <td>0.90</td>\n",
       "      <td>316.806944</td>\n",
       "      <td>42.10</td>\n",
       "      <td>-55.17</td>\n",
       "      <td>24.61</td>\n",
       "      <td>12.896</td>\n",
       "      <td>19.800467</td>\n",
       "      <td>21.739700</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>298</td>\n",
       "      <td>0.92</td>\n",
       "      <td>395.971111</td>\n",
       "      <td>42.95</td>\n",
       "      <td>-22.47</td>\n",
       "      <td>21.35</td>\n",
       "      <td>13.090</td>\n",
       "      <td>16.997767</td>\n",
       "      <td>21.457600</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>299</td>\n",
       "      <td>0.90</td>\n",
       "      <td>373.426389</td>\n",
       "      <td>40.34</td>\n",
       "      <td>-36.81</td>\n",
       "      <td>21.69</td>\n",
       "      <td>13.334</td>\n",
       "      <td>17.944000</td>\n",
       "      <td>21.798167</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>300</td>\n",
       "      <td>0.90</td>\n",
       "      <td>364.684444</td>\n",
       "      <td>42.29</td>\n",
       "      <td>-45.94</td>\n",
       "      <td>22.69</td>\n",
       "      <td>13.002</td>\n",
       "      <td>18.679800</td>\n",
       "      <td>21.801950</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5975 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      breath_id  i_time         tve  max_flow  min_flow  max_pressure    peep  \\\n",
       "0             1    0.80  545.032222     51.06    -41.03         17.37   7.600   \n",
       "1             2    0.80  531.880278     53.13    -39.97         17.13   7.508   \n",
       "2             3    0.86  523.876667     52.86    -38.24         17.11   7.658   \n",
       "3             4    0.80  507.636111     51.04    -39.37         17.14   7.572   \n",
       "4             5    0.80  518.618889     47.88    -38.51         16.92   7.598   \n",
       "...         ...     ...         ...       ...       ...           ...     ...   \n",
       "5970        296    0.90  355.365278     42.26    -51.51         23.53  13.194   \n",
       "5971        297    0.90  316.806944     42.10    -55.17         24.61  12.896   \n",
       "5972        298    0.92  395.971111     42.95    -22.47         21.35  13.090   \n",
       "5973        299    0.90  373.426389     40.34    -36.81         21.69  13.334   \n",
       "5974        300    0.90  364.684444     42.29    -45.94         22.69  13.002   \n",
       "\n",
       "         ip_auc     ep_auc  patient  \n",
       "0     11.122367  16.057733       66  \n",
       "1     11.077750  17.310533       66  \n",
       "2     12.066000  16.697800       66  \n",
       "3     11.097800  15.774250       66  \n",
       "4     11.065400  18.483333       66  \n",
       "...         ...        ...      ...  \n",
       "5970  19.216400  21.816367      662  \n",
       "5971  19.800467  21.739700      662  \n",
       "5972  16.997767  21.457600      662  \n",
       "5973  17.944000  21.798167      662  \n",
       "5974  18.679800  21.801950      662  \n",
       "\n",
       "[5975 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output some rows of the dataset just to get a better feel for the information\n",
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mznyk0N3Kzt4"
   },
   "source": [
    "Here we have 18 columns. I'm going to give a detailed breakdown here. Feel free to come back to it as necessary. \n",
    "\n",
    "Features:\n",
    " * *breath_id* - matches with a specific breath identifier from the raw data file.\n",
    " * *patient* - the patient the data came from\n",
    " * *min_flow* - The minimum flow observation on the breath\n",
    " * *max_flow* - The maximum flow observation on the breath\n",
    " * *tvi* - The inhaled volume of air for each breath\n",
    " * *tve* - The exhaled volume of air for each breath\n",
    " * *tve_tvi_ratio* - The ratio of `tve / tvi`\n",
    " * *i_time* - The amount of time patient was breathing in for each breath\n",
    " * *e_time* - The amount of time patient was breathing out for each breath\n",
    " * *ie_ratio* - The ratio of `i_time / e_time`\n",
    " * *rr* - The respiratory rate in number of breaths per minute. Measured by `60 / (i_time + e_time)`\n",
    " * *min_pressure* - the minimum pressure observation on the breath\n",
    " * *max_pressure* - the maximum pressure observation on the breath\n",
    " * *peep* - the baseline pressure setting on the ventilator\n",
    " * *pip* - the maximum pressure setting of inspiration. Slight difference compared to max_pressure\n",
    " * *maw* - the mean pressure for the entire breath\n",
    " * *ip_auc* - the area under the curve of the inspiratory pressure\n",
    " * *ep_auc* - the area under the curve of the expiratory pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOjD7m2NKzt5"
   },
   "source": [
    "## Featurization\n",
    "\n",
    "Featurization is the process where you extract information from raw data. This information can then be fed into a machine learning algorithm to perform the task you want. In the current case we will need to extract additional information from the ventilator data in order to create a valid machine learning classifier.\n",
    "\n",
    "### Processing the Data\n",
    "The first step we need to do is to be able to read the raw data files and put them into memory. We have taken this problem away from you for the purposes of this homework and have given you the code so that you can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZVmOloDxKzt5"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def process_ventilator_data(filename):\n",
    "    descriptor = open(filename)\n",
    "    reader = csv.reader(descriptor)\n",
    "    breath_id = 1\n",
    "    \n",
    "    all_breath_data = []\n",
    "    current_flow_data = []\n",
    "    current_pressure_data = []\n",
    "    \n",
    "    for row in reader:\n",
    "        if (row[0].strip() == 'BS' or row[0].strip() == 'BE') and current_flow_data != []:\n",
    "            all_breath_data.append({'breath_id': breath_id, 'flow': current_flow_data, 'pressure': current_pressure_data})\n",
    "            breath_id += 1\n",
    "            current_flow_data = []\n",
    "            current_pressure_data = []\n",
    "        else:\n",
    "            try:\n",
    "                current_flow_data.append(round(float(row[0]), 2))\n",
    "                current_pressure_data.append(round(float(row[1]), 2))\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "    return all_breath_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MRcI4TUWKzt8"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# import for Simpson's method. This will be helpful for calculating TVi\n",
    "from scipy.integrate import simps\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "def extract_features_for_file(filename, existing_features):\n",
    "    \"\"\"\n",
    "    Extract features for every single breath in file. To make matters a bit easier, we use\n",
    "    existing features that we've already extracted from the file to help speed the process.\n",
    "    \"\"\"\n",
    "    patient = filename.split('/')[-2]\n",
    "    all_breath_data = process_ventilator_data(filename)\n",
    "    all_features = []\n",
    "\n",
    "    for breath_data in all_breath_data:\n",
    "        breath_id = breath_data['breath_id']\n",
    "        existing_breath_features = existing_features[existing_features.breath_id == breath_id].iloc[0]\n",
    "\n",
    "        flow = breath_data['flow']\n",
    "        pressure = breath_data['pressure']\n",
    "        \n",
    "        # inspiratory time (the amount of time a patient is inhaling for)\n",
    "        i_time = existing_breath_features.i_time\n",
    "        # exhaled tidal volume\n",
    "        tve = existing_breath_features.tve\n",
    "        # maximum flow for breath\n",
    "        max_flow = existing_breath_features.max_flow\n",
    "        # minimum flow for the breath\n",
    "        min_flow = existing_breath_features.min_flow\n",
    "        # maximum pressure for the breath\n",
    "        max_pressure = existing_breath_features.max_pressure\n",
    "        # The minimum pressure setting on the ventilator\n",
    "        peep = existing_breath_features.peep\n",
    "        # The area under the curve of the inspiratory pressure curve\n",
    "        ip_auc = existing_breath_features.ip_auc\n",
    "        # The area under the curve of the expiratory pressure curve\n",
    "        ep_auc = existing_breath_features.ep_auc\n",
    "        \n",
    "        # This is the array index where the inhalation ends. We divide by 0.02 because\n",
    "        # thats how frequently the ventilator samples data, every 0.02 seconds.\n",
    "        x0_index = int(i_time / 0.02)\n",
    "        \n",
    "        # Part of your assignment is to extract the following features for all breaths:\n",
    "        #\n",
    "        # Expiratory Time. The amount of time a patient is exhaling\n",
    "        e_time = len(flow) * .02 - i_time\n",
    "        #\n",
    "        # I:E ratio. The ratio of inspiratory to expiratory time. Measured by i_time/e_time\n",
    "        i_e_ratio = i_time / e_time\n",
    "        #\n",
    "        # Respiratory rate. The number of breaths a patient is breathing. This is measured by\n",
    "        # 60 / (total breath time in seconds)\n",
    "        rr = 60 / (i_time + e_time)\n",
    "        rr = 60 / (len(flow) * .02)\n",
    "        #\n",
    "        # Tidal volume inhaled. The amount of air volume inhaled in the breath. \n",
    "        # Hint: use the simps function.\n",
    "        # This will output volume in L/min, convert to ml/sec (* 1000 / 60)\n",
    "        tvi = simps(flow[:x0_index], dx=0.02) * 1000 / 60\n",
    "        # \n",
    "        # Tidal volume ratio. Measured by tve/tvi\n",
    "        tve_tvi_ratio = tve / tvi\n",
    "        #\n",
    "        # Minimum pressure of the breath\n",
    "        min_pressure = min(pressure)\n",
    "        #\n",
    "        # PIP - peak inspiratory pressure. The peak pressure during inhalation\n",
    "        pip = max(pressure[:x0_index])\n",
    "        #\n",
    "        # MAW - mean airway pressure for inhalation.\n",
    "        maw = mean(pressure[:x0_index])\n",
    "        \n",
    "        all_features.append([\n",
    "            breath_id, i_time, e_time, i_e_ratio, rr, tvi, tve, tve_tvi_ratio,\n",
    "            max_flow, min_flow, max_pressure, min_pressure, pip, maw, \n",
    "            peep, ip_auc, ep_auc, int(patient) \n",
    "        ])\n",
    "    columns = [\n",
    "        'breath_id', 'i_time', 'e_time', 'i_e_ratio', 'rr', 'tvi', 'tve',\n",
    "        'tve_tvi_ratio', 'max_flow', 'min_flow', 'max_pressure',\n",
    "        'min_pressure', 'pip', 'maw', 'peep', 'ip_auc', 'ep_auc', 'patient'\n",
    "    ]\n",
    "    return all_features, columns\n",
    "\n",
    "\n",
    "def remake_dataset(dataset):\n",
    "    data_files = glob(os.path.join('data', '*/*.csv'))\n",
    "    \n",
    "    patient_to_file_map = {}\n",
    "    for filename in data_files:\n",
    "        patient = filename.split('/')[-2]  # patient is embedded in this part of filename\n",
    "        patient_to_file_map[patient] = filename\n",
    "\n",
    "    data = []\n",
    "    # iterate over all the unique patients in the train set\n",
    "    for patient in dataset.patient.unique():\n",
    "        existing_features = dataset[dataset.patient == patient]\n",
    "        filename = patient_to_file_map[str(patient)]\n",
    "        breath_data, columns = extract_features_for_file(filename, existing_features)\n",
    "        # add breath rows\n",
    "        data.extend(breath_data)\n",
    "    # create new data frame with the new added information\n",
    "    return pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SIFPJtktKzt-",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'66'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-6e3d7a386fc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# remake train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# remake validation set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-d5e9f62635f2>\u001b[0m in \u001b[0;36mremake_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpatient\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mexisting_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpatient\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatient_to_file_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mbreath_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features_for_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# add breath rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '66'"
     ]
    }
   ],
   "source": [
    "# remake train set\n",
    "train_x = remake_dataset(train_x)\n",
    "# remake validation set.\n",
    "test_x = remake_dataset(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Xnjp0FmuKzuA",
    "outputId": "44336221-6fe4-42bf-a7be-4b7155991b50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breath_id</th>\n",
       "      <th>i_time</th>\n",
       "      <th>tve</th>\n",
       "      <th>max_flow</th>\n",
       "      <th>min_flow</th>\n",
       "      <th>max_pressure</th>\n",
       "      <th>peep</th>\n",
       "      <th>ip_auc</th>\n",
       "      <th>ep_auc</th>\n",
       "      <th>patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>545.032222</td>\n",
       "      <td>51.06</td>\n",
       "      <td>-41.03</td>\n",
       "      <td>17.37</td>\n",
       "      <td>7.600</td>\n",
       "      <td>11.122367</td>\n",
       "      <td>16.057733</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>531.880278</td>\n",
       "      <td>53.13</td>\n",
       "      <td>-39.97</td>\n",
       "      <td>17.13</td>\n",
       "      <td>7.508</td>\n",
       "      <td>11.077750</td>\n",
       "      <td>17.310533</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>523.876667</td>\n",
       "      <td>52.86</td>\n",
       "      <td>-38.24</td>\n",
       "      <td>17.11</td>\n",
       "      <td>7.658</td>\n",
       "      <td>12.066000</td>\n",
       "      <td>16.697800</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.80</td>\n",
       "      <td>507.636111</td>\n",
       "      <td>51.04</td>\n",
       "      <td>-39.37</td>\n",
       "      <td>17.14</td>\n",
       "      <td>7.572</td>\n",
       "      <td>11.097800</td>\n",
       "      <td>15.774250</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>518.618889</td>\n",
       "      <td>47.88</td>\n",
       "      <td>-38.51</td>\n",
       "      <td>16.92</td>\n",
       "      <td>7.598</td>\n",
       "      <td>11.065400</td>\n",
       "      <td>18.483333</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>296</td>\n",
       "      <td>0.90</td>\n",
       "      <td>355.365278</td>\n",
       "      <td>42.26</td>\n",
       "      <td>-51.51</td>\n",
       "      <td>23.53</td>\n",
       "      <td>13.194</td>\n",
       "      <td>19.216400</td>\n",
       "      <td>21.816367</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>297</td>\n",
       "      <td>0.90</td>\n",
       "      <td>316.806944</td>\n",
       "      <td>42.10</td>\n",
       "      <td>-55.17</td>\n",
       "      <td>24.61</td>\n",
       "      <td>12.896</td>\n",
       "      <td>19.800467</td>\n",
       "      <td>21.739700</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>298</td>\n",
       "      <td>0.92</td>\n",
       "      <td>395.971111</td>\n",
       "      <td>42.95</td>\n",
       "      <td>-22.47</td>\n",
       "      <td>21.35</td>\n",
       "      <td>13.090</td>\n",
       "      <td>16.997767</td>\n",
       "      <td>21.457600</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>299</td>\n",
       "      <td>0.90</td>\n",
       "      <td>373.426389</td>\n",
       "      <td>40.34</td>\n",
       "      <td>-36.81</td>\n",
       "      <td>21.69</td>\n",
       "      <td>13.334</td>\n",
       "      <td>17.944000</td>\n",
       "      <td>21.798167</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>300</td>\n",
       "      <td>0.90</td>\n",
       "      <td>364.684444</td>\n",
       "      <td>42.29</td>\n",
       "      <td>-45.94</td>\n",
       "      <td>22.69</td>\n",
       "      <td>13.002</td>\n",
       "      <td>18.679800</td>\n",
       "      <td>21.801950</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5975 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      breath_id  i_time         tve  max_flow  min_flow  max_pressure    peep  \\\n",
       "0             1    0.80  545.032222     51.06    -41.03         17.37   7.600   \n",
       "1             2    0.80  531.880278     53.13    -39.97         17.13   7.508   \n",
       "2             3    0.86  523.876667     52.86    -38.24         17.11   7.658   \n",
       "3             4    0.80  507.636111     51.04    -39.37         17.14   7.572   \n",
       "4             5    0.80  518.618889     47.88    -38.51         16.92   7.598   \n",
       "...         ...     ...         ...       ...       ...           ...     ...   \n",
       "5970        296    0.90  355.365278     42.26    -51.51         23.53  13.194   \n",
       "5971        297    0.90  316.806944     42.10    -55.17         24.61  12.896   \n",
       "5972        298    0.92  395.971111     42.95    -22.47         21.35  13.090   \n",
       "5973        299    0.90  373.426389     40.34    -36.81         21.69  13.334   \n",
       "5974        300    0.90  364.684444     42.29    -45.94         22.69  13.002   \n",
       "\n",
       "         ip_auc     ep_auc  patient  \n",
       "0     11.122367  16.057733       66  \n",
       "1     11.077750  17.310533       66  \n",
       "2     12.066000  16.697800       66  \n",
       "3     11.097800  15.774250       66  \n",
       "4     11.065400  18.483333       66  \n",
       "...         ...        ...      ...  \n",
       "5970  19.216400  21.816367      662  \n",
       "5971  19.800467  21.739700      662  \n",
       "5972  16.997767  21.457600      662  \n",
       "5973  17.944000  21.798167      662  \n",
       "5974  18.679800  21.801950      662  \n",
       "\n",
       "[5975 rows x 10 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilPoq-riKzuD"
   },
   "source": [
    "### Create Ground Truth (that the machine understands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "CZyvPnXVKzuE",
    "outputId": "cb746889-51e6-46f0-f1e4-3231901b4ad4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breath_id</th>\n",
       "      <th>patient</th>\n",
       "      <th>bsa</th>\n",
       "      <th>dta</th>\n",
       "      <th>cough</th>\n",
       "      <th>suction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>295</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>296</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>297</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>298</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>299</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1247 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      breath_id  patient  bsa  dta  cough  suction\n",
       "0            20      292    1    0      0        0\n",
       "1            21      292    1    0      0        0\n",
       "2            22      292    0    0      0        0\n",
       "3            23      292    1    0      0        0\n",
       "4            24      292    1    0      0        0\n",
       "...         ...      ...  ...  ...    ...      ...\n",
       "1242        295      114    0    0      0        0\n",
       "1243        296      114    0    0      0        0\n",
       "1244        297      114    0    0      0        0\n",
       "1245        298      114    0    0      0        0\n",
       "1246        299      114    0    0      0        0\n",
       "\n",
       "[1247 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the test dataset and set it up. Technically we're using the validation set.\n",
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtS2Cl7NKzuG"
   },
   "source": [
    "What does this mean?\n",
    "\n",
    "We have 6 columns here\n",
    " * *breath_id* - matches with a specific breath identifier from the raw data file.\n",
    " * *patient* - the patient the data came from\n",
    " * *bsa* - Breath Stacking Asynchrony. A single breath where the patient is trapping air in their chest\n",
    " * *dta* - Double Trigger Asynchrony. Two breaths in a row where the patient is trapping air\n",
    " * *cough* - What it sounds like, when a patient coughs\n",
    " * *suction* -  Nurses perform suction procedures to remove excess fluid from an endotracheal tube. This waveform is indicative of that. \n",
    " \n",
    "Now that we understand what our columns are, we need to put it into a format where the machine can understand it and create a learning model. Because this is a multiclass model, let's just have non-PVA breaths be class 0, breath stacking can be class 1, double trigger can be class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zE44UfwtKzuG",
    "outputId": "68fede4c-4368-452f-f70c-2b61548af3a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1242    0\n",
       "1243    0\n",
       "1244    0\n",
       "1245    0\n",
       "1246    0\n",
       "Length: 1247, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a multi-class y vector that we can use for training purposes. \n",
    "train_y_vector = train_y.bsa * 1 + train_y.dta * 2\n",
    "test_y_vector = test_y.bsa * 1 + test_y.dta * 2\n",
    "test_y_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qzrRJ5-SKzuJ",
    "outputId": "ef2dea76-f6c2-448d-81e5-46c47a172adf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5438    3\n",
       "5440    3\n",
       "5521    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if there places where the data was mis-annotated, where both double trigger and breath stack was annotated.\n",
    "# It's just good to know if this is happening or not so that we can either drop the data, or change it later on.\n",
    "train_y_vector[train_y_vector > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8FoDLUZKzuM"
   },
   "source": [
    "### Creating a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "Th1VdBy0KzuM"
   },
   "outputs": [],
   "source": [
    "# Need to finalize dataset and remove misannotated examples first.\n",
    "\n",
    "# just drop places where data is double annotated. \n",
    "misannotated_train = train_y_vector > 2\n",
    "misannotated_test = test_y_vector > 2\n",
    "\n",
    "# ~ is the NOT operator\n",
    "train_x = train_x.loc[~misannotated_train]\n",
    "train_y_vector = train_y_vector.loc[~misannotated_train]\n",
    "\n",
    "# do same thing for test \n",
    "test_x = test_x.loc[~misannotated_test]\n",
    "test_y_vector = test_y_vector.loc[~misannotated_test]\n",
    "\n",
    "\n",
    "\n",
    "# Also make sure to drop data that is NaN. This is very important because otherwise your model won't train.\n",
    "# The .any(axis=1) function basically says, if there are any nans in this *ROW* then mark the row as true.\n",
    "# The .any(axis=0) would mark columns as True/False, but this isn't helpful now.\n",
    "nans_train = train_x.isna().any(axis=1)\n",
    "nans_test = test_x.isna().any(axis=1)\n",
    "\n",
    "# now filter them out of the dataset in the same way\n",
    "train_x = train_x.loc[~nans_train]\n",
    "train_y_vector = train_y_vector.loc[~nans_train]\n",
    "\n",
    "test_x = test_x.loc[~nans_test]\n",
    "test_y_vector = test_y_vector.loc[~nans_test]\n",
    "\n",
    "\n",
    "# any time we drop things from a data frame or series in pandas it is often helpful to re-index the object.\n",
    "# the index is usually a sequential ordering of the rows like 1, 2, ... n. Sometimes it can be different\n",
    "# but for now we'll just use sequential ordering\n",
    "train_x.index = range(len(train_x))\n",
    "train_y_vector.index = range(len(train_y_vector))\n",
    "\n",
    "test_x.index = range(len(test_x))\n",
    "test_y_vector.index = range(len(test_y_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "PCCAgbrGKzuP",
    "outputId": "e5a501b8-42b8-4686-93cb-eb528aebf654"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breath_id</th>\n",
       "      <th>i_time</th>\n",
       "      <th>tve</th>\n",
       "      <th>max_flow</th>\n",
       "      <th>min_flow</th>\n",
       "      <th>max_pressure</th>\n",
       "      <th>peep</th>\n",
       "      <th>ip_auc</th>\n",
       "      <th>ep_auc</th>\n",
       "      <th>patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>545.032222</td>\n",
       "      <td>51.06</td>\n",
       "      <td>-41.03</td>\n",
       "      <td>17.37</td>\n",
       "      <td>7.600</td>\n",
       "      <td>11.122367</td>\n",
       "      <td>16.057733</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>531.880278</td>\n",
       "      <td>53.13</td>\n",
       "      <td>-39.97</td>\n",
       "      <td>17.13</td>\n",
       "      <td>7.508</td>\n",
       "      <td>11.077750</td>\n",
       "      <td>17.310533</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>523.876667</td>\n",
       "      <td>52.86</td>\n",
       "      <td>-38.24</td>\n",
       "      <td>17.11</td>\n",
       "      <td>7.658</td>\n",
       "      <td>12.066000</td>\n",
       "      <td>16.697800</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.80</td>\n",
       "      <td>507.636111</td>\n",
       "      <td>51.04</td>\n",
       "      <td>-39.37</td>\n",
       "      <td>17.14</td>\n",
       "      <td>7.572</td>\n",
       "      <td>11.097800</td>\n",
       "      <td>15.774250</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>518.618889</td>\n",
       "      <td>47.88</td>\n",
       "      <td>-38.51</td>\n",
       "      <td>16.92</td>\n",
       "      <td>7.598</td>\n",
       "      <td>11.065400</td>\n",
       "      <td>18.483333</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>296</td>\n",
       "      <td>0.90</td>\n",
       "      <td>355.365278</td>\n",
       "      <td>42.26</td>\n",
       "      <td>-51.51</td>\n",
       "      <td>23.53</td>\n",
       "      <td>13.194</td>\n",
       "      <td>19.216400</td>\n",
       "      <td>21.816367</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>297</td>\n",
       "      <td>0.90</td>\n",
       "      <td>316.806944</td>\n",
       "      <td>42.10</td>\n",
       "      <td>-55.17</td>\n",
       "      <td>24.61</td>\n",
       "      <td>12.896</td>\n",
       "      <td>19.800467</td>\n",
       "      <td>21.739700</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5969</th>\n",
       "      <td>298</td>\n",
       "      <td>0.92</td>\n",
       "      <td>395.971111</td>\n",
       "      <td>42.95</td>\n",
       "      <td>-22.47</td>\n",
       "      <td>21.35</td>\n",
       "      <td>13.090</td>\n",
       "      <td>16.997767</td>\n",
       "      <td>21.457600</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>299</td>\n",
       "      <td>0.90</td>\n",
       "      <td>373.426389</td>\n",
       "      <td>40.34</td>\n",
       "      <td>-36.81</td>\n",
       "      <td>21.69</td>\n",
       "      <td>13.334</td>\n",
       "      <td>17.944000</td>\n",
       "      <td>21.798167</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>300</td>\n",
       "      <td>0.90</td>\n",
       "      <td>364.684444</td>\n",
       "      <td>42.29</td>\n",
       "      <td>-45.94</td>\n",
       "      <td>22.69</td>\n",
       "      <td>13.002</td>\n",
       "      <td>18.679800</td>\n",
       "      <td>21.801950</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5972 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      breath_id  i_time         tve  max_flow  min_flow  max_pressure    peep  \\\n",
       "0             1    0.80  545.032222     51.06    -41.03         17.37   7.600   \n",
       "1             2    0.80  531.880278     53.13    -39.97         17.13   7.508   \n",
       "2             3    0.86  523.876667     52.86    -38.24         17.11   7.658   \n",
       "3             4    0.80  507.636111     51.04    -39.37         17.14   7.572   \n",
       "4             5    0.80  518.618889     47.88    -38.51         16.92   7.598   \n",
       "...         ...     ...         ...       ...       ...           ...     ...   \n",
       "5967        296    0.90  355.365278     42.26    -51.51         23.53  13.194   \n",
       "5968        297    0.90  316.806944     42.10    -55.17         24.61  12.896   \n",
       "5969        298    0.92  395.971111     42.95    -22.47         21.35  13.090   \n",
       "5970        299    0.90  373.426389     40.34    -36.81         21.69  13.334   \n",
       "5971        300    0.90  364.684444     42.29    -45.94         22.69  13.002   \n",
       "\n",
       "         ip_auc     ep_auc  patient  \n",
       "0     11.122367  16.057733       66  \n",
       "1     11.077750  17.310533       66  \n",
       "2     12.066000  16.697800       66  \n",
       "3     11.097800  15.774250       66  \n",
       "4     11.065400  18.483333       66  \n",
       "...         ...        ...      ...  \n",
       "5967  19.216400  21.816367      662  \n",
       "5968  19.800467  21.739700      662  \n",
       "5969  16.997767  21.457600      662  \n",
       "5970  17.944000  21.798167      662  \n",
       "5971  18.679800  21.801950      662  \n",
       "\n",
       "[5972 rows x 10 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV5sP7CLKzun"
   },
   "source": [
    "__From Lab2, we know there are only 3 classes in our labels as shown below.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nVdi-lDPKzun",
    "outputId": "a92ad0ff-deb2-47a6-fb7b-5e176bdc7e3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breath_id</th>\n",
       "      <th>patient</th>\n",
       "      <th>bsa</th>\n",
       "      <th>dta</th>\n",
       "      <th>cough</th>\n",
       "      <th>suction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>295</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>296</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>297</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>298</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>299</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1247 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      breath_id  patient  bsa  dta  cough  suction\n",
       "0            20      292    1    0      0        0\n",
       "1            21      292    1    0      0        0\n",
       "2            22      292    0    0      0        0\n",
       "3            23      292    1    0      0        0\n",
       "4            24      292    1    0      0        0\n",
       "...         ...      ...  ...  ...    ...      ...\n",
       "1242        295      114    0    0      0        0\n",
       "1243        296      114    0    0      0        0\n",
       "1244        297      114    0    0      0        0\n",
       "1245        298      114    0    0      0        0\n",
       "1246        299      114    0    0      0        0\n",
       "\n",
       "[1247 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNrSsxopKzuu"
   },
   "source": [
    "__What does this mean?__\n",
    "\n",
    "We have 6 columns here\n",
    " * *breath_id* - matches with a specific breath identifier from the raw data file.\n",
    " * *patient* - the patient the data came from\n",
    " * *bsa* - Breath Stacking Asynchrony. A single breath where the patient is trapping air in their chest\n",
    " * *dta* - Double Trigger Asynchrony. Two breaths in a row where the patient is trapping air\n",
    " * *cough* - What it sounds like, when a patient coughs\n",
    " * *suction* -  Nurses perform suction procedures to remove excess fluid from an endotracheal tube. This waveform is indicative of that. \n",
    " \n",
    "Now that we understand what our columns are, we need to put it into a format where the machine can understand it and create a learning model. Because this is a multiclass model, let's just have __non-PVA breaths be class 0, breath stacking can be class 1, double trigger can be class 2__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdoQLqpLKxxT"
   },
   "source": [
    "## Assignment \\#1 Scaling\n",
    "It's often helpful to have data scaled into a certain range of values. For neural networks it is essential, and for random forests it very frequently helps improve performance. There are multiple different ways you can scale your data. \n",
    "\n",
    "#### Standardization\n",
    "A popular method is standardization where you subtract the mean of feature and then divide by its standard deviation. \n",
    "\n",
    "$(x_f - \\mu_f) \\div \\sigma_f$\n",
    "\n",
    "Where $x_f$ is the feature vector, or more simply, a single column in the pandas dataframe.\n",
    "$\\mu_f$ is the mean of the feature vector. Which can also be computed in pandas via `data_frame.column_name.mean()`\n",
    "$\\sigma_f$ is the standard deviation of the feature vector. Which can be computed `data_frame.column_name.std()`. \n",
    "\n",
    "Scikit-Learn has a class to do this that also saves your coefficients.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# initialize scaler with default parameters. To play around with class options check out the scikit-learn \n",
    "# documentation at https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to training data, and then scale the training data.\n",
    "train_set_scaled = scaler.fit_transform(train_set)\n",
    "# Transform testing data based on fitted model.\n",
    "#\n",
    "# note the difference here! We don't fit our scaler to the test set because this could bias our model.\n",
    "test_set_scaled = scaler.transform(test_set)\n",
    "```\n",
    "\n",
    "#### Min-Max\n",
    "Min max scaling natively scales all feature vectors between 0 and 1. The math doing this is again pretty simple.\n",
    "\n",
    "$(x_f - min(x_f)) \\div (max(x_f) - min(x_f))$\n",
    "\n",
    "Where the `min` function is just finding the minimum value of a feature vector, and the `max` function is finding the maximum value of a feature vector. You can do this quickly in Scikit-Learn too.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_set_scaled = scaler.fit_transform(train_set)\n",
    "test_set_scaled = scaler.transform(test_set)\n",
    "```\n",
    "\n",
    "#### Robust Scaler\n",
    "Probably the most advanced out of these scalers (but not necessarily better), the robust scaler removes the median of the feature vector, and then scales it according to a quantile range (by default the IQR). This scaler is strong if your data has large amounts of outliers. Different models may also be good with different scalers. Sometimes it is helpful just to play around with your model and see what works.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "train_set_scaled = scaler.fit_transform(train_set)\n",
    "test_set_scaled = scaler.transform(test_set)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE8bSTJqT3nk"
   },
   "source": [
    "### Implement these three scaling methods separately on your Random Forest, the model you selected in Assignment 3 from Lab 2, compare and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.65      0.73       842\n",
      "           1       0.43      0.84      0.57       301\n",
      "           2       1.00      0.02      0.04       104\n",
      "\n",
      "    accuracy                           0.64      1247\n",
      "   macro avg       0.76      0.50      0.45      1247\n",
      "weighted avg       0.76      0.64      0.64      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#Random Forest with StandardScaler\n",
    "#######################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "train_set_scaled= scaler.fit_transform(train_x[columns_to_use])\n",
    "test_set_scaled = scaler.transform(test_x[columns_to_use])\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# don't use patient and breath_id columns\n",
    "#columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "\n",
    "# fit the model to training \n",
    "model.fit(train_set_scaled, train_y_vector)\n",
    "\n",
    "# Now that the model is fitted, evaluate how well it is performing\n",
    "predictions = model.predict(test_set_scaled)\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.64      0.73       842\n",
      "           1       0.39      0.81      0.53       301\n",
      "           2       0.50      0.02      0.04       104\n",
      "\n",
      "    accuracy                           0.63      1247\n",
      "   macro avg       0.59      0.49      0.43      1247\n",
      "weighted avg       0.72      0.63      0.63      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#####Logistic Regression with StandardScaler\n",
    "#######################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "# fit the model to training \n",
    "modelLR.fit(train_set_scaled, train_y_vector)\n",
    "\n",
    "# Now that the model is fitted, evaluate how well it is performing\n",
    "predictions = modelLR.predict(test_set_scaled)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.64      0.72       842\n",
      "           1       0.42      0.85      0.57       301\n",
      "           2       1.00      0.02      0.04       104\n",
      "\n",
      "    accuracy                           0.64      1247\n",
      "   macro avg       0.75      0.50      0.44      1247\n",
      "weighted avg       0.75      0.64      0.63      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#Random Forest with Min-Max\n",
    "#######################################\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_set_scaled = scaler.fit_transform(train_x)\n",
    "test_set_scaled = scaler.transform(test_x)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# don't use patient and breath_id columns\n",
    "#columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "\n",
    "# fit the model to training \n",
    "model.fit(train_set_scaled, train_y_vector)\n",
    "\n",
    "# Now that the model is fitted, evaluate how well it is performing\n",
    "predictions = model.predict(test_set_scaled)\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.60      0.71       842\n",
      "           1       0.37      0.84      0.52       301\n",
      "           2       0.00      0.00      0.00       104\n",
      "\n",
      "    accuracy                           0.61      1247\n",
      "   macro avg       0.42      0.48      0.41      1247\n",
      "weighted avg       0.69      0.61      0.61      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#Logistic Regression with Min-Max\n",
    "#######################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "# fit the model to training \n",
    "modelLR.fit(train_set_scaled, train_y_vector)\n",
    "\n",
    "# Now that the model is fitted, evaluate how well it is performing\n",
    "predictions = modelLR.predict(test_set_scaled)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74       842\n",
      "           1       0.44      0.85      0.58       301\n",
      "           2       1.00      0.02      0.04       104\n",
      "\n",
      "    accuracy                           0.65      1247\n",
      "   macro avg       0.76      0.51      0.45      1247\n",
      "weighted avg       0.76      0.65      0.64      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#Random Forest with Robust Scaler\n",
    "#####################################\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "train_set_scaled = scaler.fit_transform(train_x)\n",
    "test_set_scaled = scaler.transform(test_x)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# don't use patient and breath_id columns\n",
    "#columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "\n",
    "# fit the model to training \n",
    "model.fit(train_set_scaled, train_y_vector)\n",
    "\n",
    "# Now that the model is fitted, evaluate how well it is performing\n",
    "predictions = model.predict(test_set_scaled)\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.57      0.69       842\n",
      "           1       0.36      0.84      0.51       301\n",
      "           2       0.55      0.06      0.10       104\n",
      "\n",
      "    accuracy                           0.59      1247\n",
      "   macro avg       0.60      0.49      0.43      1247\n",
      "weighted avg       0.73      0.59      0.60      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#Logistic Regression with Robust Scaler\n",
    "#####################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "# fit the model to training \n",
    "modelLR.fit(train_set_scaled, train_y_vector)\n",
    "\n",
    "# Now that the model is fitted, evaluate how well it is performing\n",
    "predictions = modelLR.predict(test_set_scaled)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DCY16oHKxxY"
   },
   "source": [
    "## Assignment \\#2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFr2IcciKxxZ"
   },
   "source": [
    "One thing to note is that we are using 16 different features for input into our model. Some of these features can be of little value to classifying whether a breath is asynchronous or not. So, one of the easiest things we can do for ourselves is to reduce the number of features that we have in an intelligent way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-SP0V3yKxxZ"
   },
   "source": [
    "### $\\chi^2$ Feature Selection (chi squared)\n",
    "\n",
    "Probably one of the easiest methods and intuitive methods to use for feature selection in classification problems. The [$\\chi^2$ test](https://en.wikipedia.org/wiki/Chi-squared_test) measures whether a two statistical distributions are independent. In t[he applied case](https://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html), this means asking the question of whether a single feature is independent of the target vector. If a feature and the outcome are independent then this variable might not be helpful for our model. If a feature is independent of the outcome it will have a high chi2 value and a high pvalue. On the other hand, if a feature is not independent of the outcome, then it will have a high chi2 value and a low p-value (within range of 0-.05).\n",
    "\n",
    "There is a function in scikit-learn that enables you to do the $\\chi^2$ test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "POGQw_D2KxxZ",
    "outputId": "a3c97638-5d85-4d42-a5a1-1b9aee1f934f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0845956610568945e-15, 'ep_auc'),\n",
      " (4.1963626094160874e-10, 'tve'),\n",
      " (0.01875342252606717, 'i_time'),\n",
      " (0.05675212148048651, 'ip_auc'),\n",
      " (0.15346609771450404, 'max_pressure'),\n",
      " (0.4176834544812503, 'min_flow'),\n",
      " (0.4539978193631702, 'peep'),\n",
      " (0.6801265398971507, 'max_flow')]\n"
     ]
    }
   ],
   "source": [
    "# this is the PrettyPrint function. Just makes things look a bit nicer on output.\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# get all columns in our dataset except patient and breath_id\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "\n",
    "# must scale feature vectors so they are non-negative\n",
    "scaler = MinMaxScaler()\n",
    "train_set = scaler.fit_transform(train_x[columns_to_use])\n",
    "\n",
    "# the chi2 test will output two things, chi2 and p values. The p values are the most relevant item that we want\n",
    "# to use. A feature with a p-value between 0 and 0.05 means that a feature might be a good predictor of our outcome.\n",
    "chi2_vals, pvals = chi2(train_set, train_y_vector)\n",
    "\n",
    "# mash column names with p-values so we know which p-value belongs to which feature\n",
    "cols_to_pvals = zip(pvals, columns_to_use)\n",
    "# Sort the p-values in ascending order (smallest first).\n",
    "cols_sorted = sorted(cols_to_pvals)\n",
    "# pretty print the sorted values.\n",
    "pprint(cols_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwzpIgrEKxxb"
   },
   "source": [
    "There are 2 features that had p-values below 0.05:\n",
    "\n",
    " * tve \n",
    " * ep_auc \n",
    " \n",
    "So let's use these features for our next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "F3DhMwx8Kxxc",
    "outputId": "d4aa6046-acca-4757-b953-44ffb7943b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77       842\n",
      "           1       0.40      0.48      0.44       301\n",
      "           2       0.07      0.02      0.03       104\n",
      "\n",
      "    accuracy                           0.64      1247\n",
      "   macro avg       0.41      0.43      0.41      1247\n",
      "weighted avg       0.62      0.64      0.63      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "columns_to_use = ['tve', 'ep_auc']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_set = scaler.fit_transform(train_x[columns_to_use])\n",
    "test_set = scaler.transform(test_x[columns_to_use])\n",
    "\n",
    "model.fit(train_set, train_y_vector)\n",
    "predictions = model.predict(test_set)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z79abeYmKxxi"
   },
   "source": [
    "Our performance actually dropped when we were trying to use $\\chi^2$ test. Does this mean that the $\\chi^2$ method isn't good for our problem?\n",
    "\n",
    "What is happening above?\n",
    "\n",
    "Even though the $\\chi^2$ test is telling us these features are relevant to prediction, this just isn't the case in  the test set. This can happen frequently in machine learning, where information that is relevant to the training set doesn't generalize to the testing set. Are there other methods of feature selection which are more likely to generalize to the testing set?\n",
    "\n",
    "### Expert Feature Selection\n",
    "\n",
    "It always helps to have expert knowledge on the problem to improve model performance. In this case expert knowledge can be considered medical knowledge. So what kind of medical knowledge can we use to help this?\n",
    "\n",
    "#### Breath Stack (BSA)\n",
    "Remember the waveforms here? This means that the patient is trapping air in their chest. We can measure this via the `tve_tvi_ratio`. The way that our doctors annotated breaths was if the breaths had a `tve_tvi_ratio < .9` and they weren't a suction/cough or another anomaly.\n",
    "\n",
    "<img src=\"bsa-breath.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### Double Trigger (DTA)\n",
    "Double trigger has a double-hump pattern to it. \n",
    "\n",
    "<img src=\"dta-breaths.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The way our doctors annotated it was if \n",
    "\n",
    "1. It wasn't an anomaly\n",
    "2. First breath in sequence had an `e_time < .3` seconds\n",
    "3. First breath in sequence had `tve_tvi_ratio < .25` OR first breath had `0.25 <= tve_tvi_ratio < 0.5` and `tve < 100`\n",
    "\n",
    "Knowing this which features can we use here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.50      0.61       842\n",
      "           1       0.33      0.76      0.46       301\n",
      "           2       0.17      0.02      0.03       104\n",
      "\n",
      "    accuracy                           0.52      1247\n",
      "   macro avg       0.43      0.43      0.37      1247\n",
      "weighted avg       0.62      0.52      0.53      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#Assignment2\n",
    "############################################\n",
    "\n",
    "#Expert Feature Selection\n",
    "model = RandomForestClassifier()\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', \n",
    "                                                       'breath_id'], \n",
    "                                                      ['tve'],\n",
    "                                                     ['max_flow'], \n",
    "                                                      ['min_flow'], \n",
    "                                                      ['max_pressure'], \n",
    "                                                      ['peep'], \n",
    "                                                      ['breath_id'], \n",
    "                                                      ['i_time']))\n",
    "\n",
    "# pick features based on expert selection. left for reader to determine best columns\n",
    "train_set = train_x[columns_to_use]\n",
    "test_set = test_x[columns_to_use]\n",
    "\n",
    "model.fit(train_set, train_y_vector)\n",
    "predictions = model.predict(test_set)\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rd3c4XwKxxk"
   },
   "source": [
    "### Other Methods\n",
    "\n",
    "You are welcome to use other methods / mathematical functions for feature selection as well. I will briefly outline some of them here.\n",
    "\n",
    "\n",
    "#### Wrapper Methods\n",
    "\n",
    "This performs feature selection by brute force. Using your validation set, train many models with every single possible feature combination you can have. Determination of which features work best can be chosen based on the best performing model. Then you can apply this model to your testing set to determine performance. \n",
    "\n",
    "Pros:\n",
    " * easy to understand\n",
    " * easy to code\n",
    " \n",
    "Cons:\n",
    " * prone to overfitting\n",
    " * is time consuming. Must train $n!$ models if $n$ is the number of features.\n",
    " \n",
    "#### PCA (Principal Component Analysis)\n",
    "\n",
    "This method utilizes the [principal component analysis algorithm](https://en.wikipedia.org/wiki/Principal_component_analysis) to transform your dataset and generate new features that are independent of each other. The user gets to choose the number of features that are generated, and often modelers choose to generate an increasing number of features, and then train a new model for each PCA run while determining the performance of each model.\n",
    "\n",
    "Pros:\n",
    " * Dimensionality reduction will cause models to train faster\n",
    " * Generated features are linearly uncorrelated with each other\n",
    " * Easy to utilize because there are multiple existing functions for this, like in [sckit-learn.](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    " \n",
    "Cons:\n",
    " * Loss of information in your data will likely occur and may cause performance degradation\n",
    " * Human comprehension of features is lost when PCA is performed\n",
    "\n",
    "#### Mutual Information\n",
    "\n",
    "[Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) is similar to $\\chi^2$ feature selection and measures the dependency between two variables. For machine learning this dependency can be measured between a feature and the target. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "Pros:\n",
    " * Fast\n",
    " * Supported by [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#r50b872b699c4-1)\n",
    " \n",
    "Cons:\n",
    " * Like $\\chi^2$ may not generalize to the test set.\n",
    "\n",
    "#### Mixed Methods\n",
    "\n",
    "It is possible use a variety of methods in combination with each other. Generally expert feature selection is the first method used and then additional synthetic methods are added on top of this. Ultimately as the modeler, this work is on you to figure out how to perform best. One method might work for one problem and then completely fail for another. This is why it is often best to utilize as many possible methods as possible when performing modeling and only declare a winner when all other possible methods have been explored. It is critical to always beware of overfitting. Have a good validation set to evaluate your model, and don't pick your best methods versus your testing set. This is almost guaranteed to lead to overfitting.\n",
    "\n",
    "### Finish Expert Feature Selection & Find another Feature Selection Method to Use.\n",
    "\n",
    "Finish the coding for expert feature selection and use another feature selection method like PCA/mutual information/wrapper methods for use in your model. Which one performs best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5SJXb66Kxxl"
   },
   "outputs": [],
   "source": [
    "# XXX code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljwceFfoT3nw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeQlqvtrKxxm"
   },
   "source": [
    "## Other Ways to Improve Your Model\n",
    "\n",
    "### Class Imbalance\n",
    "\n",
    "Class imbalance occurs when one class comprises a larger ratio of the observations in the dataset than another. This can be seen very clearly in our current training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNFP_mRjKxxn",
    "outputId": "d616feb6-f5aa-46e8-c696-9f5d6f3ff063"
   },
   "outputs": [],
   "source": [
    "train_y_vector.value_counts() / len(train_y_vector) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYyxs9StKxxo"
   },
   "source": [
    "We can see here that normal observations comprise 72.3% of our training dataset, BSA is 19.68%, and DTA is 8.01%. This imbalance can have implications on the training of machine learning models because our model may not have enough information to learn effective class boundaries. Some algorithms are more resistant to class imbalance than others. Neural networks however are particularly affected by imbalance issues because of the nature of the way training is performed with these algorithms. Often algorithms besides neural networks benefit from techniques to reduce the class imbalance issue too. There are a number of techniques to tackle class imbalance.\n",
    "\n",
    "#### ROS (Random Over-Sampling)\n",
    "\n",
    "Random over-sampling aims to oversample minority classes by choosing observations at random with replacement until we  meet a certain ratio of majority to minority class observations. This is a fairly easy thing to code yourself if you wanted to do it, but just for ease we're going to use the [imbalanced-learn python package.](https://imbalanced-learn.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "KLOJBsmmKxxp",
    "outputId": "44d82912-aa39-48ea-c775-fa76d348b3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.63      0.73       842\n",
      "           1       0.43      0.88      0.57       301\n",
      "           2       1.00      0.13      0.24       104\n",
      "\n",
      "    accuracy                           0.65      1247\n",
      "   macro avg       0.76      0.55      0.51      1247\n",
      "weighted avg       0.77      0.65      0.65      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "\n",
    "# get all columns in our dataset except patient and breath_id\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "# Initialize the RandomOverSampler. This initialization will give us 1:1:1 class ratios. If we want different\n",
    "# ratios then we can chance the sampling_strategy input argument. For more details see the documentation\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler\n",
    "ros = imblearn.over_sampling.RandomOverSampler()\n",
    "# re-sample the train set ONLY. Don't resample the testing set because otherwise you would be biasing model conclusions\n",
    "train_x_ros, train_y_ros = ros.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "\n",
    "# put the target vector into a series so we can just do some convenience function.\n",
    "train_y_ros = pd.Series(train_y_ros)\n",
    "# You'll see the dataset is equilibrated now with equal observations normal, BSA, and DTA breaths.\n",
    "train_y_ros.value_counts()\n",
    "\n",
    "# Now we can put this back into our model and see if performance changes. This is left for the reader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBmeV_-PKxxr"
   },
   "source": [
    "#### RUS (Random Under-Sampling)\n",
    "\n",
    "![](over-sampling-undersampling.png)\n",
    "\n",
    "Random under-sampling is basically the inverse of the over-sampling technique. Instead of selecting with replacement from minority classes, here we randomly sample from the majority classes only until they meet some class ratio with the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "6AmL8yxjKxxs",
    "outputId": "937b2440-4dd5-4c27-d2de-3285782fcb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.53      0.66       842\n",
      "           1       0.41      0.80      0.54       301\n",
      "           2       0.17      0.25      0.21       104\n",
      "\n",
      "    accuracy                           0.57      1247\n",
      "   macro avg       0.48      0.53      0.47      1247\n",
      "weighted avg       0.70      0.57      0.59      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "\n",
    "# get all columns in our dataset except patient and breath_id\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "# Initialize the RandomUnderSampler. This initialization will give us 1:1:1 class ratios. If we want different\n",
    "# ratios then we can chance the sampling_strategy input argument. For more details see the documentation\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler\n",
    "rus = imblearn.under_sampling.RandomUnderSampler()\n",
    "# re-sample the train set ONLY. Don't resample the testing set because otherwise you would be biasing model conclusions\n",
    "train_x_rus, train_y_rus = rus.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "\n",
    "# put the target vector into a series so we can just do some convenience function.\n",
    "train_y_rus = pd.Series(train_y_rus)\n",
    "# You'll see the dataset is equilibrated now with equal observations normal, BSA, and DTA breaths.\n",
    "train_y_rus.value_counts()\n",
    "\n",
    "# Now we can put this back into our model and see if performance changes. This is left for the reader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNTlpc7TKxxu"
   },
   "source": [
    "There are some downsides to RUS in that we are discarding data from the majority class which might be useful for the future. Also if some classes have very low ratios of data relative to the majority class then RUS may have more limited utility. With RUS, as with ROS, we will need to evaluate the effect of different class ratios on our validation set. Maybe a `4:2:1` ratio would be best for this problem, we just don't know until we try. I will leave this as an additional exercise for the reader.\n",
    "\n",
    "#### SMOTE (Synthetic Minority Oversampling TEchnique)\n",
    "\n",
    "One downside about the methods mentioned is that they always are drawn from the existing distribution of class data. It is quite possible that if we collected additional samples that there would be new observations that fit in between these existing data points. This is the intuition behind smote that can also be seen in the below image.\n",
    "\n",
    "![](smote-intuition.png)\n",
    "\n",
    "The benefit of SMOTE is that we are expanding our dataset, which means more data for our model to train on, while we are semi-intelligently generating new samples. Of course generated data may have no basis for reality, so good modeling habit should always check to see whether RUS, ROS, or SMOTE works best for a problem, and which class ratios work best for which technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "M6EIip_zKxxu",
    "outputId": "607378d7-06e2-4b81-deaa-383eefcbc109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.15      0.25       842\n",
      "           1       0.39      0.76      0.52       301\n",
      "           2       0.06      0.27      0.10       104\n",
      "\n",
      "    accuracy                           0.31      1247\n",
      "   macro avg       0.38      0.39      0.29      1247\n",
      "weighted avg       0.57      0.31      0.30      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import imblearn \n",
    "\n",
    "# get all columns in our dataset except patient and breath_id\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "# Initialize SMOTE. This initialization will give us 1:1:1 class ratios. If we want different\n",
    "# ratios then we can chance the sampling_strategy input argument. For more details see the documentation\n",
    "# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE\n",
    "smote = imblearn.over_sampling.SMOTE()\n",
    "# re-sample the train set ONLY. Don't resample the testing set because otherwise you would be biasing model conclusions\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "\n",
    "# put the target vector into a series so we can just do some convenience function.\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "# You'll see the dataset is equilibrated now with equal observations normal, BSA, and DTA breaths.\n",
    "train_y_smote.value_counts()\n",
    "\n",
    "\n",
    "# Now we can put this back into our model and see if performance changes. This is left for the reader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL8JjUwNKxxy"
   },
   "source": [
    "## Assignment \\#3 Utilize all 3 Imbalance Correction Techniques \n",
    "\n",
    "Utilize ROS, RUS, and SMOTE with the following imbalance ratios: 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0. Is there an algorithm that performs best? Are there ratios of imbalance that perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "PSSdsqqfKxxy"
   },
   "outputs": [],
   "source": [
    "# Example code for creating 0.3 imbalance ratio. The same parameters will work for ROS and RUS functions too.\n",
    "##################################0.3 for SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.3)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.3 for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.3)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.3 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.3)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYVwsZrBT3n_"
   },
   "outputs": [],
   "source": [
    "##################################0.4 SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.4)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.4 for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.4)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.4 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.4)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################0.5 SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.5)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.5 for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.5)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.5 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.5)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################0.6 SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.6)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.6for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.6)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.6 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.6)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################0.7 SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.7)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.7 for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.7)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.7 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.7)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################0.8SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.8)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.8for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.8)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.8 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.8)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################0.9SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.9)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################0.9 for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.9)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################0.9for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 0.9)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################1.0 SMOTE\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 1.0)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_smote, train_y_smote)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2\n",
    "        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "##################################1.0 for ROS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 1.0)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_ros, train_y_ros)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2       \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))\n",
    "\n",
    "##################################1.0 for RUS\n",
    "import imblearn \n",
    "columns_to_use = list(set(train_x.columns).difference(['patient', 'breath_id']))\n",
    "smote = imblearn.over_sampling.SMOTE(sampling_strategy = 1.0)\n",
    "train_x_smote, train_y_smote = smote.fit_resample(train_x[columns_to_use], train_y_vector)\n",
    "train_y_smote = pd.Series(train_y_smote)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_x_rus, train_y_rus)\n",
    "predictions = model.predict(test_x[columns_to_use])\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == 2:\n",
    "        predictions[idx-1] = 2        \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y_vector, predictions))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mini_Project_B1_Improve_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
